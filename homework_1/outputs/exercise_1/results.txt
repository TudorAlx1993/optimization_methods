My implementations:
	* method=constant_step
		* convergence after no_iterations=482
		* x_star=[ 0.21007091 -0.27926984  0.0885046   0.07997591  0.01205794  0.06279622
 -0.10040899 -0.22436542 -0.63454373  0.16251538 -0.11409997  0.26317324
 -0.26942409 -0.08043037  0.25594667  0.19465819 -0.08738734  0.09830062
 -0.13504547 -0.09847198  0.34926694  0.05985052 -0.17131443 -0.04136034
 -0.15720043  0.20923827  0.46660867  0.11626582 -0.1052274   0.23450043
 -0.06028815  0.03184557 -0.31476082 -0.20109255 -0.07358472 -0.0542332
  0.25926666 -0.42342367  0.01193571 -0.02036841  0.13991621 -0.38399687
  0.13128797 -0.25468445 -0.08839286 -0.2688035  -0.01005588  0.04791733
  0.05651962  0.29619588]
		* gradient norm of f(x_star)=9.855168724516068e-06
		* objective function in x_star=11.106495001842115
	* method=ideal_step
		* convergence after no_iterations=399
		* x_star=[ 0.21007091 -0.27926984  0.08850459  0.0799759   0.01205795  0.06279622
 -0.10040899 -0.22436541 -0.63454373  0.16251537 -0.11409996  0.26317323
 -0.26942409 -0.08043037  0.25594666  0.19465818 -0.08738735  0.09830062
 -0.13504547 -0.09847198  0.34926695  0.05985052 -0.17131443 -0.04136034
 -0.15720043  0.20923827  0.46660866  0.11626582 -0.1052274   0.23450043
 -0.06028815  0.03184557 -0.31476082 -0.20109255 -0.07358472 -0.0542332
  0.25926666 -0.42342366  0.01193571 -0.02036841  0.1399162  -0.38399686
  0.13128797 -0.25468445 -0.08839286 -0.2688035  -0.01005588  0.04791734
  0.05651962  0.29619587]
		* gradient norm of f(x_star)=9.953170685093354e-06
		* objective function in x_star=11.106495001842395
	* method=adaptive_step
		* convergence after no_iterations=120
		* x_star=[ 0.21007077 -0.27926967  0.08850456  0.07997588  0.01205769  0.0627959
 -0.10040918 -0.22436588 -0.63454376  0.16251563 -0.11409991  0.26317331
 -0.26942379 -0.08043052  0.25594681  0.19465813 -0.08738719  0.09830068
 -0.13504532 -0.09847217  0.34926686  0.05985057 -0.17131463 -0.04136025
 -0.15720017  0.20923816  0.46660907  0.11626573 -0.10522752  0.23450042
 -0.06028831  0.03184575 -0.31476088 -0.20109254 -0.07358471 -0.05423299
  0.25926653 -0.42342383  0.01193575 -0.0203683   0.13991644 -0.38399666
  0.1312878  -0.25468452 -0.08839313 -0.26880329 -0.01005613  0.04791716
  0.05651952  0.29619603]
		* gradient norm of f(x_star)=8.02156107862707e-06
		* objective function in x_star=11.106495001836205
	* numpy implementation
		* x_star=[ 0.21007057 -0.27926987  0.08850472  0.07997622  0.0120576   0.06279584
 -0.10040935 -0.22436643 -0.63454414  0.1625158  -0.11410005  0.26317349
 -0.26942379 -0.08043078  0.25594715  0.19465858 -0.08738669  0.09830075
 -0.13504542 -0.09847226  0.34926679  0.05985093 -0.17131469 -0.04136011
 -0.15720038  0.2092383   0.46661004  0.11626569 -0.10522761  0.23449992
 -0.06028822  0.0318456  -0.31476097 -0.20109268 -0.07358475 -0.05423304
  0.25926644 -0.42342405  0.01193561 -0.02036798  0.13991665 -0.38399704
  0.1312873  -0.25468406 -0.08839373 -0.26880346 -0.01005599  0.04791695
  0.05651944  0.29619629]
		* gradient norm of f(x_star)=1.63808468285561e-13
		* objective function in x_star=11.10649500182808
