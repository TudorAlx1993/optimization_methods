My implementations:
	* method=constant_step
		* convergence after no_iterations=634
		* x_star=[ 0.04206188 -0.14379952 -0.07572331 -0.14388776 -0.0352083  -0.1806261
  0.18704383 -0.30949755  0.13101897  0.20465847 -0.09670216  0.37896457
  0.1548372  -0.13548999  0.09092269 -0.25505421 -0.12799503  0.00427029
  0.09577536 -0.36799469  0.09760448 -0.08178969  0.09784554  0.21015296
 -0.12643513 -0.03934517 -0.04090691 -0.16992763  0.33155298  0.38082651
  0.00888366 -0.13018566 -0.18630596 -0.02294875  0.04749389 -0.02480637
 -0.16624265 -0.31664707  0.41626805  0.39197995 -0.26335707 -0.00571275
  0.28279935  0.12979326 -0.17490709 -0.50067606 -0.05488195  0.13511274
  0.14971705  0.06346937]
		* gradient norm of f(x_star)=9.93708459920507e-06
		* objective function in x_star=13.179168132327934
	* method=ideal_step
		* convergence after no_iterations=614
		* x_star=[ 0.04206189 -0.14379952 -0.07572332 -0.14388776 -0.0352083  -0.18062609
  0.18704383 -0.30949754  0.13101896  0.20465847 -0.09670216  0.37896457
  0.15483721 -0.13548998  0.09092268 -0.25505421 -0.12799503  0.0042703
  0.09577535 -0.36799469  0.09760448 -0.08178968  0.09784553  0.21015296
 -0.12643513 -0.03934517 -0.04090692 -0.16992764  0.33155298  0.3808265
  0.00888366 -0.13018566 -0.18630595 -0.02294875  0.04749388 -0.02480636
 -0.16624265 -0.31664706  0.41626805  0.39197994 -0.26335706 -0.00571275
  0.28279934  0.12979326 -0.17490708 -0.50067605 -0.05488194  0.13511273
  0.14971704  0.06346937]
		* gradient norm of f(x_star)=9.834211272102216e-06
		* objective function in x_star=13.179168132327538
	* method=adaptive_step
		* convergence after no_iterations=175
		* x_star=[ 0.042062   -0.14379955 -0.07572341 -0.14388778 -0.03520831 -0.18062598
  0.18704383 -0.30949744  0.13101894  0.20465837 -0.09670213  0.37896455
  0.1548374  -0.13548989  0.09092255 -0.25505421 -0.12799501  0.00427036
  0.09577525 -0.36799465  0.0976044  -0.08178966  0.09784549  0.21015295
 -0.12643507 -0.03934517 -0.04090699 -0.16992765  0.33155294  0.38082643
  0.00888361 -0.13018564 -0.18630588 -0.02294872  0.04749379 -0.02480623
 -0.16624267 -0.31664693  0.41626802  0.39197985 -0.26335693 -0.00571276
  0.28279924  0.12979325 -0.17490699 -0.50067601 -0.05488179  0.13511271
  0.14971694  0.06346938]
		* gradient norm of f(x_star)=8.46901035450596e-06
		* objective function in x_star=13.179168132322626
	* numpy implementation
		* x_star=[ 0.04206268 -0.1437997  -0.07572399 -0.1438879  -0.03520833 -0.18062532
  0.18704382 -0.30949683  0.13101881  0.20465775 -0.09670195  0.37896444
  0.15483856 -0.13548932  0.09092178 -0.25505423 -0.12799489  0.00427075
  0.09577464 -0.36799441  0.09760392 -0.08178952  0.0978452   0.21015293
 -0.12643475 -0.03934512 -0.04090744 -0.16992775  0.33155269  0.38082596
  0.00888331 -0.13018556 -0.18630546 -0.02294853  0.04749321 -0.02480543
 -0.16624274 -0.31664614  0.41626785  0.39197932 -0.26335615 -0.00571282
  0.28279859  0.12979318 -0.17490643 -0.50067577 -0.05488092  0.13511253
  0.1497163   0.06346945]
		* gradient norm of f(x_star)=1.2394443747737212e-13
		* objective function in x_star=13.179168132308842
