My implementations:
	* method=constant_step
		* convergence after no_iterations=404
		* x_star=[ 0.04124743 -0.20273168  0.25922055  0.00238402 -0.13392505  0.13736842
  0.10372429  0.24498024  0.05443     0.08760868 -0.2136341  -0.0892657
 -0.12841836 -0.21973528 -0.29313972  0.00261156  0.03863645 -0.07401356
  0.12561532 -0.08375478  0.14895118 -0.08888828 -0.0021397  -0.27759383
 -0.13187338  0.20549383  0.3340413   0.07156729 -0.29235063 -0.23435448
  0.01431043 -0.21343857 -0.17573121  0.08093416  0.11885201 -0.1168905
 -0.11769716 -0.05747824  0.12909059  0.00110055 -0.02607695  0.16292009
  0.03090633 -0.14093508  0.12163166  0.08159484 -0.13780548  0.07333116
 -0.14205525 -0.07876837]
		* gradient norm of f(x_star)=9.725151849371818e-06
		* objective function in x_star=9.168399474439653
	* method=ideal_step
		* convergence after no_iterations=374
		* x_star=[ 0.04124744 -0.20273169  0.25922055  0.00238403 -0.13392504  0.13736842
  0.10372429  0.24498023  0.05443001  0.08760867 -0.21363409 -0.0892657
 -0.12841835 -0.21973528 -0.29313971  0.00261156  0.03863644 -0.07401357
  0.12561532 -0.08375479  0.14895118 -0.08888828 -0.0021397  -0.27759383
 -0.13187338  0.20549384  0.33404131  0.07156729 -0.29235062 -0.23435447
  0.01431044 -0.21343858 -0.17573121  0.08093417  0.11885201 -0.1168905
 -0.11769715 -0.05747824  0.12909059  0.00110055 -0.02607696  0.1629201
  0.03090633 -0.14093508  0.12163165  0.08159485 -0.13780548  0.07333116
 -0.14205524 -0.07876836]
		* gradient norm of f(x_star)=9.90787061405685e-06
		* objective function in x_star=9.168399474440086
	* method=adaptive_step
		* convergence after no_iterations=138
		* x_star=[ 0.04124735 -0.20273165  0.25922055  0.00238402 -0.13392507  0.13736839
  0.10372432  0.24498028  0.05442998  0.08760869 -0.21363415 -0.08926569
 -0.12841837 -0.21973533 -0.29313978  0.00261152  0.0386365  -0.0740135
  0.12561528 -0.08375473  0.14895115 -0.0888883  -0.00213975 -0.27759382
 -0.13187335  0.20549382  0.3340413   0.07156729 -0.29235067 -0.23435453
  0.01431041 -0.21343853 -0.17573125  0.08093411  0.11885196 -0.11689053
 -0.11769716 -0.05747826  0.12909061  0.00110052 -0.02607695  0.16292006
  0.03090631 -0.14093506  0.1216317   0.08159481 -0.13780551  0.07333118
 -0.14205527 -0.07876842]
		* gradient norm of f(x_star)=9.344308047607017e-06
		* objective function in x_star=9.168399474437877
	* implementation with scipy.optimize.minimize
		* x_star=[ 0.04124703 -0.20273139  0.25922053  0.00238379 -0.13392524  0.13736812
  0.10372456  0.24498099  0.05442957  0.08760908 -0.21363453 -0.08926581
 -0.12841855 -0.21973585 -0.29314026  0.00261137  0.03863717 -0.07401308
  0.12561501 -0.08375433  0.14895105 -0.08888844 -0.00214002 -0.27759359
 -0.13187292  0.20549352  0.33404116  0.0715673  -0.29235127 -0.23435507
  0.01431021 -0.21343796 -0.17573151  0.08093365  0.11885163 -0.11689073
 -0.11769739 -0.05747825  0.12909058  0.00110028 -0.0260768   0.1629197
  0.03090626 -0.14093502  0.12163238  0.08159435 -0.13780565  0.07333129
 -0.1420557  -0.07876869]
		* gradient norm of f(x_star)=6.771576625664981e-06
		* objective function in x_star=9.168399474429304
